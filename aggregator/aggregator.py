"""
Sitbrief Aggregator - æ™ºåº«/åª’é«”é¦–é æ¨™é¡Œèšåˆå™¨
æ”¯æ´ç¶²é æŠ“å–å’Œ RSS Feed å…©ç¨®æ¨¡å¼
"""

import asyncio
import json
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timezone
from pathlib import Path
from urllib.parse import urljoin, urlparse

import httpx
import yaml
from playwright.async_api import async_playwright


class HeadlineAggregator:
    def __init__(self, config_path: str = "sources.yaml"):
        self.config_path = Path(config_path)
        self.sources = self._load_sources()
        self.output_dir = Path(__file__).parent / "output"
        self.output_dir.mkdir(exist_ok=True)

    def _load_sources(self) -> list:
        with open(self.config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        return config.get("sources", [])

    async def fetch_rss(self, source: dict) -> list:
        """å¾ RSS Feed æŠ“å–æ¨™é¡Œ"""
        headlines = []
        name = source["name"]
        feeds = source.get("feeds", [])
        
        print(f"ğŸ“¡ æŠ“å– {name} (RSS)...")
        
        async with httpx.AsyncClient() as client:
            for feed_url in feeds:
                try:
                    response = await client.get(feed_url, timeout=30)
                    response.raise_for_status()
                    
                    root = ET.fromstring(response.text)
                    
                    # è™•ç† Atom feed
                    ns = {'atom': 'http://www.w3.org/2005/Atom'}
                    entries = root.findall('.//atom:entry', ns)
                    
                    for entry in entries[:20]:  # æ¯å€‹ feed æœ€å¤š 20 å‰‡
                        title_elem = entry.find('atom:title', ns)
                        link_elem = entry.find('atom:link[@rel="alternate"]', ns)
                        
                        if title_elem is not None and link_elem is not None:
                            title = title_elem.text.strip() if title_elem.text else ""
                            url = link_elem.get('href', '')
                            
                            if title and url:
                                headlines.append({
                                    "title": title,
                                    "url": url,
                                    "source": name
                                })
                                
                except Exception as e:
                    print(f"  âš ï¸ RSS æŠ“å–å¤±æ•— {feed_url}: {e}")
        
        # å»é‡
        seen = set()
        unique = []
        for h in headlines:
            if h['url'] not in seen:
                seen.add(h['url'])
                unique.append(h)
        
        print(f"  âœ… å–å¾— {len(unique)} å‰‡æ¨™é¡Œ")
        return unique

    async def fetch_web(self, source: dict) -> list:
        """æŠ“å–å–®ä¸€ä¾†æºçš„æ¨™é¡Œ"""
        headlines = []
        name = source["name"]
        url = source["url"]
        selectors = source.get("selectors", {})
        exclude_patterns = selectors.get("exclude", [])

        print(f"ğŸ“¡ æŠ“å– {name}...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                await page.goto(url, wait_until="networkidle", timeout=30000)
                
                # å–å¾—æ‰€æœ‰æ–‡ç« é€£çµ - æ”¯æ´å¤šå€‹é¸æ“‡å™¨
                article_selector = selectors.get("articles", "a")
                
                # å¦‚æœé¸æ“‡å™¨åŒ…å«é€—è™Ÿï¼Œåˆ†åˆ¥è™•ç†
                all_links = []
                for selector in article_selector.split(","):
                    selector = selector.strip()
                    try:
                        links = await page.query_selector_all(selector)
                        all_links.extend(links)
                    except Exception:
                        pass
                
                seen_urls = set()
                
                for link in all_links:
                    try:
                        href = await link.get_attribute("href")
                        text = await link.inner_text()
                        text = text.strip()
                        
                        if not href or not text:
                            continue
                        
                        # è½‰æ›ç›¸å°è·¯å¾‘
                        full_url = urljoin(url, href)
                        
                        # æª¢æŸ¥æ’é™¤æ¨¡å¼
                        if any(pattern in full_url for pattern in exclude_patterns):
                            continue
                        
                        # å»é‡
                        if full_url in seen_urls:
                            continue
                        seen_urls.add(full_url)
                        
                        # æ¸…ç†æ¨™é¡Œï¼ˆç§»é™¤å¤šé¤˜ç©ºç™½ï¼‰
                        text = re.sub(r'\s+', ' ', text).strip()
                        
                        # éæ¿¾å¤ªçŸ­çš„æ¨™é¡Œ
                        if len(text) < 10:
                            continue
                        
                        headlines.append({
                            "title": text,
                            "url": full_url,
                            "source": name
                        })
                        
                    except Exception as e:
                        continue
                
            except Exception as e:
                print(f"  âŒ æŠ“å–å¤±æ•—: {e}")
            finally:
                await browser.close()
        
        print(f"  âœ… å–å¾— {len(headlines)} å‰‡æ¨™é¡Œ")
        return headlines

    async def fetch_source(self, source: dict) -> list:
        """æ ¹æ“šé¡å‹é¸æ“‡æŠ“å–æ–¹å¼"""
        source_type = source.get("type", "web")
        if source_type == "rss":
            return await self.fetch_rss(source)
        else:
            return await self.fetch_web(source)

    async def fetch_all(self) -> dict:
        """æŠ“å–æ‰€æœ‰ä¾†æº"""
        all_headlines = []
        
        for source in self.sources:
            headlines = await self.fetch_source(source)
            all_headlines.extend(headlines)
        
        result = {
            "fetchedAt": datetime.now(timezone.utc).isoformat(),
            "totalCount": len(all_headlines),
            "sources": [s["name"] for s in self.sources],
            "headlines": all_headlines
        }
        
        return result

    def save_result(self, result: dict, filename: str = "headlines.json"):
        """å„²å­˜çµæœ"""
        output_path = self.output_dir / filename
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ å·²å„²å­˜åˆ° {output_path}")
        return output_path

    def save_html(self, result: dict, filename: str = "headlines.html"):
        """å„²å­˜ HTML æ ¼å¼çµæœ"""
        output_path = self.output_dir / filename
        
        # æŒ‰ä¾†æºåˆ†çµ„
        by_source = {}
        for h in result["headlines"]:
            source = h["source"]
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(h)
        
        # æ ¼å¼åŒ–æ™‚é–“
        fetched_at = result["fetchedAt"][:19].replace("T", " ")
        
        html = f'''<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sitbrief Headlines - {fetched_at}</title>
    <style>
        * {{ box-sizing: border-box; margin: 0; padding: 0; }}
        body {{ 
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            background: #f5f5f7;
            color: #1d1d1f;
            line-height: 1.5;
            padding: 20px;
        }}
        .container {{ max-width: 900px; margin: 0 auto; }}
        header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 12px;
            margin-bottom: 24px;
        }}
        header h1 {{ font-size: 28px; margin-bottom: 8px; }}
        header .meta {{ opacity: 0.9; font-size: 14px; }}
        .source-section {{
            background: white;
            border-radius: 12px;
            padding: 24px;
            margin-bottom: 20px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
        }}
        .source-section h2 {{
            font-size: 18px;
            color: #667eea;
            margin-bottom: 16px;
            padding-bottom: 8px;
            border-bottom: 2px solid #f0f0f0;
        }}
        .source-section h2 span {{
            background: #667eea;
            color: white;
            font-size: 12px;
            padding: 2px 8px;
            border-radius: 12px;
            margin-left: 8px;
        }}
        ul {{ list-style: none; }}
        li {{ 
            padding: 12px 0;
            border-bottom: 1px solid #f0f0f0;
        }}
        li:last-child {{ border-bottom: none; }}
        a {{ 
            color: #1d1d1f;
            text-decoration: none;
            display: block;
        }}
        a:hover {{ color: #667eea; }}
        .url {{
            font-size: 12px;
            color: #86868b;
            margin-top: 4px;
            word-break: break-all;
        }}
        footer {{
            text-align: center;
            padding: 20px;
            color: #86868b;
            font-size: 12px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ“° Sitbrief Headlines</h1>
            <div class="meta">
                æ›´æ–°æ™‚é–“ï¼š{fetched_at} UTC<br>
                ä¾†æºï¼š{', '.join(result['sources'])} | å…± {result['totalCount']} å‰‡
            </div>
        </header>
'''
        
        for source, headlines in by_source.items():
            html += f'''
        <section class="source-section">
            <h2>{source} <span>{len(headlines)}</span></h2>
            <ul>
'''
            for h in headlines:
                title = h["title"].replace("<", "&lt;").replace(">", "&gt;")
                url = h["url"]
                domain = urlparse(url).netloc
                html += f'''                <li>
                    <a href="{url}" target="_blank">{title}</a>
                    <div class="url">{domain}</div>
                </li>
'''
            html += '''            </ul>
        </section>
'''
        
        html += '''
        <footer>
            Generated by Sitbrief Aggregator
        </footer>
    </div>
</body>
</html>
'''
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html)
        print(f"ğŸ“„ å·²å„²å­˜ HTML åˆ° {output_path}")
        return output_path

    def print_summary(self, result: dict, limit: int = 10):
        """å°å‡ºæ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“° èšåˆçµæœæ‘˜è¦")
        print(f"{'='*60}")
        print(f"æŠ“å–æ™‚é–“: {result['fetchedAt']}")
        print(f"ä¾†æºæ•¸é‡: {len(result['sources'])}")
        print(f"ç¸½æ¨™é¡Œæ•¸: {result['totalCount']}")
        print(f"\næœ€æ–° {limit} å‰‡æ¨™é¡Œï¼š")
        print("-" * 60)
        
        for i, h in enumerate(result["headlines"][:limit], 1):
            title = h["title"][:50] + "..." if len(h["title"]) > 50 else h["title"]
            print(f"{i:2}. [{h['source']}] {title}")


async def main():
    aggregator = HeadlineAggregator()
    result = await aggregator.fetch_all()
    aggregator.save_result(result)
    aggregator.save_html(result)
    aggregator.print_summary(result)


if __name__ == "__main__":
    asyncio.run(main())
